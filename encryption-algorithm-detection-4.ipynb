{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9386710,"sourceType":"datasetVersion","datasetId":5695387}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-09-17T13:02:35.602333Z","iopub.execute_input":"2024-09-17T13:02:35.602632Z","iopub.status.idle":"2024-09-17T13:02:37.302596Z","shell.execute_reply.started":"2024-09-17T13:02:35.602598Z","shell.execute_reply":"2024-09-17T13:02:37.301636Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/encryption-algos/output.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-17T13:02:37.303684Z","iopub.execute_input":"2024-09-17T13:02:37.304071Z","iopub.status.idle":"2024-09-17T13:02:38.389434Z","shell.execute_reply.started":"2024-09-17T13:02:37.304038Z","shell.execute_reply":"2024-09-17T13:02:38.388636Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-17T13:02:38.390647Z","iopub.execute_input":"2024-09-17T13:02:38.391086Z","iopub.status.idle":"2024-09-17T13:02:38.413241Z","shell.execute_reply.started":"2024-09-17T13:02:38.391038Z","shell.execute_reply":"2024-09-17T13:02:38.412326Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                 KEY  \\\n0                   1d9650cd94d15814db78257b98b7845f   \n1  68b0284fd58244c86cfde403c1a27fb5bdb1d3a2c9a04e...   \n2  ec86b3d7d4c01d11477fd5a18b71d0ddf36405445641a8...   \n3  d070fe3955ecf07bc269d9f0c9cd7b6f8299c4361f3d80...   \n4   14e9580736248f7697b540ec0a8f3164facfd8a6369db2d5   \n\n                                 IV                     NONCE  \\\n0  9944c5bf5d78a73e140fa86210634687                       NaN   \n1                               NaN          40b11ab4f67f4f66   \n2                               NaN  675d38594ac0ea8e65cbdc25   \n3                               NaN          ddb5b3e5f1148bb5   \n4                  5520e558b3efc1fa                       NaN   \n\n                                                  CT          ALGORITHM  \\\n0  5ae7ab9406ead8e35eeca5a95e9f1d74807b1dc49b676e...                AES   \n1  8db28f48a79a3a6c7bd70bbe5a8b6271051fdc7bd8df81...           ChaCha20   \n2  ebed76e47179abbfb9920dfb57ea45b76ae77c6a3191bd...  ChaCha20_Poly1305   \n3  ddb5b3e5f1148bb513bfe4d3879b59f6a07c463b4a1322...            Salsa20   \n4  5520e558b3efc1fa18d951a35f38843f389d276273a308...               3DES   \n\n                                       ORIGINAL TEXT  \n0  Today arrived with a crash of my car through t...  \n1  Today arrived with a crash of my car through t...  \n2  Today arrived with a crash of my car through t...  \n3  Today arrived with a crash of my car through t...  \n4  Today arrived with a crash of my car through t...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>KEY</th>\n      <th>IV</th>\n      <th>NONCE</th>\n      <th>CT</th>\n      <th>ALGORITHM</th>\n      <th>ORIGINAL TEXT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1d9650cd94d15814db78257b98b7845f</td>\n      <td>9944c5bf5d78a73e140fa86210634687</td>\n      <td>NaN</td>\n      <td>5ae7ab9406ead8e35eeca5a95e9f1d74807b1dc49b676e...</td>\n      <td>AES</td>\n      <td>Today arrived with a crash of my car through t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>68b0284fd58244c86cfde403c1a27fb5bdb1d3a2c9a04e...</td>\n      <td>NaN</td>\n      <td>40b11ab4f67f4f66</td>\n      <td>8db28f48a79a3a6c7bd70bbe5a8b6271051fdc7bd8df81...</td>\n      <td>ChaCha20</td>\n      <td>Today arrived with a crash of my car through t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ec86b3d7d4c01d11477fd5a18b71d0ddf36405445641a8...</td>\n      <td>NaN</td>\n      <td>675d38594ac0ea8e65cbdc25</td>\n      <td>ebed76e47179abbfb9920dfb57ea45b76ae77c6a3191bd...</td>\n      <td>ChaCha20_Poly1305</td>\n      <td>Today arrived with a crash of my car through t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d070fe3955ecf07bc269d9f0c9cd7b6f8299c4361f3d80...</td>\n      <td>NaN</td>\n      <td>ddb5b3e5f1148bb5</td>\n      <td>ddb5b3e5f1148bb513bfe4d3879b59f6a07c463b4a1322...</td>\n      <td>Salsa20</td>\n      <td>Today arrived with a crash of my car through t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14e9580736248f7697b540ec0a8f3164facfd8a6369db2d5</td>\n      <td>5520e558b3efc1fa</td>\n      <td>NaN</td>\n      <td>5520e558b3efc1fa18d951a35f38843f389d276273a308...</td>\n      <td>3DES</td>\n      <td>Today arrived with a crash of my car through t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-17T13:02:38.415290Z","iopub.execute_input":"2024-09-17T13:02:38.415621Z","iopub.status.idle":"2024-09-17T13:02:38.532760Z","shell.execute_reply.started":"2024-09-17T13:02:38.415585Z","shell.execute_reply":"2024-09-17T13:02:38.531821Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 189371 entries, 0 to 189370\nData columns (total 6 columns):\n #   Column         Non-Null Count   Dtype \n---  ------         --------------   ----- \n 0   KEY            189371 non-null  object\n 1   IV             108212 non-null  object\n 2   NONCE          81159 non-null   object\n 3   CT             189371 non-null  object\n 4   ALGORITHM      189371 non-null  object\n 5   ORIGINAL TEXT  189357 non-null  object\ndtypes: object(6)\nmemory usage: 8.7+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# For data preparation and visualization\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T13:02:38.533991Z","iopub.execute_input":"2024-09-17T13:02:38.534838Z","iopub.status.idle":"2024-09-17T13:02:41.998230Z","shell.execute_reply.started":"2024-09-17T13:02:38.534793Z","shell.execute_reply":"2024-09-17T13:02:41.997434Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n\n# Convert hex to numeric values for the 'KEY', 'IV', 'NONCE', and 'CT' columns\ndef hex_to_numeric(hex_string):\n    if pd.isna(hex_string):  # Handle NaN values\n        return []\n    return [int(c, 16) for c in hex_string]\n\n# Pad sequences to the maximum length\ndef pad_sequences(sequences, maxlen): \n    return np.array([seq + [0] * (maxlen - len(seq)) if len(seq) < maxlen else seq[:maxlen] for seq in sequences])\n\n# Apply hex conversion to the 'KEY', 'IV', 'NONCE', and 'CT' columns\ndf['key_numeric'] = df['KEY'].apply(hex_to_numeric)\ndf['iv_numeric'] = df['IV'].apply(hex_to_numeric)\ndf['nonce_numeric'] = df['NONCE'].apply(hex_to_numeric)\ndf['ct_numeric'] = df['CT'].apply(hex_to_numeric)\n\n# Concatenate the numeric values of KEY, IV, NONCE, and CT\ndf['combined_numeric'] = df['key_numeric'] + df['iv_numeric'] + df['nonce_numeric'] + df['ct_numeric']\n\n# Get max sequence length from all combined numeric columns\nmaxlen = max(df['combined_numeric'].apply(len))\n\n# Pad the combined sequences\nX = pad_sequences(df['combined_numeric'].tolist(), maxlen)\n\n# One-hot encode the 'ALGORITHM' column\nencoder = OneHotEncoder(sparse_output=False)\ny = encoder.fit_transform(df[['ALGORITHM']])\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.long)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n\n# Create DataLoader for batching\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\nbatch_size = 512\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Print information to check if everything works\nprint(f\"Train data shape: {X_train_tensor.shape}, Train labels shape: {y_train_tensor.shape}\")\nprint(f\"Test data shape: {X_test_tensor.shape}, Test labels shape: {y_test_tensor.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T13:02:41.999404Z","iopub.execute_input":"2024-09-17T13:02:41.999847Z","iopub.status.idle":"2024-09-17T13:02:58.389205Z","shell.execute_reply.started":"2024-09-17T13:02:41.999814Z","shell.execute_reply":"2024-09-17T13:02:58.388292Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Train data shape: torch.Size([151496, 448]), Train labels shape: torch.Size([151496, 7])\nTest data shape: torch.Size([37875, 448]), Test labels shape: torch.Size([37875, 7])\n","output_type":"stream"}]},{"cell_type":"code","source":"\nclass PositionalEncoding(nn.Module):\n    def __init__(self, embed_dim, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.encoding = torch.zeros(max_len, embed_dim)\n        positions = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n        self.encoding[:, 0::2] = torch.sin(positions * div_term)\n        self.encoding[:, 1::2] = torch.cos(positions * div_term)\n        self.encoding = self.encoding.unsqueeze(0)\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        return x + self.encoding[:, :seq_len, :].to(x.device)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_dim, num_classes, embed_dim=64, num_heads=4, ff_dim=128, num_layers=2, dropout=0.2):\n        super(TransformerModel, self).__init__()\n        self.embedding = nn.Embedding(input_dim, embed_dim)  # Input dim reflects the size of hex values\n        self.positional_encoding = PositionalEncoding(embed_dim)\n\n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n\n        # Fully connected layers tailored to your dataset\n        self.fc1 = nn.Linear(embed_dim, 512)   # Fully connected layer 1\n        self.fc2 = nn.Linear(512, 256)         # Fully connected layer 2\n        self.fc3 = nn.Linear(256, 128)         # Fully connected layer 3\n        self.fc4 = nn.Linear(128, 64)          # Fully connected layer 4\n        self.fc5 = nn.Linear(64, num_classes)  # Output layer\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Input embedding and positional encoding\n        x = self.embedding(x)\n        x = self.positional_encoding(x)\n        \n        # Pass through Transformer encoder\n        x = self.transformer_encoder(x)\n        \n        # Global average pooling\n        x = torch.mean(x, dim=1)  \n\n        # Pass through fully connected layers\n        x = self.dropout(torch.relu(self.fc1(x)))\n        x = self.dropout(torch.relu(self.fc2(x)))\n        x = self.dropout(torch.relu(self.fc3(x)))\n        x = self.dropout(torch.relu(self.fc4(x)))\n        x = self.fc5(x)  # No activation here, handled by loss function\n\n        return x\n\n# Define input/output dimensions\n# Input dimension is 16 because hex digits range from 0-9 and a-f (16 possible characters)\ninput_dim = 16  # For hexadecimal characters (0-9, a-f)\nnum_classes = len(df['ALGORITHM'].unique())  # Number of unique algorithms as target classes\n\n# Create the model\nmodel = TransformerModel(input_dim=input_dim, num_classes=num_classes)\n\n# Use GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T13:02:58.721755Z","iopub.execute_input":"2024-09-17T13:02:58.722072Z","iopub.status.idle":"2024-09-17T13:02:58.760192Z","shell.execute_reply.started":"2024-09-17T13:02:58.722039Z","shell.execute_reply":"2024-09-17T13:02:58.759280Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\ndef train(model, train_loader, criterion, optimizer, num_epochs):\n    model.train()\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        # Loop over batches\n        for X_batch, y_batch in train_loader:\n            # Move data to the correct device (GPU or CPU)\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            \n            # Forward pass: compute the model output\n            outputs = model(X_batch)\n            \n            # Convert one-hot encoded labels to class indices for accuracy calculation\n            _, labels = torch.max(y_batch, 1)\n            \n            # Compute the loss (CrossEntropyLoss expects class indices)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            # Accumulate loss\n            total_loss += loss.item()\n            \n            # Accuracy computation\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n        \n        # Compute average loss and accuracy for the epoch\n        avg_loss = total_loss / len(train_loader)\n        accuracy = correct / total * 100\n        \n        # Print epoch details\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n\n# Train the model\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ntrain(model, train_loader, criterion, optimizer, num_epochs=250)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T13:02:58.761664Z","iopub.execute_input":"2024-09-17T13:02:58.762176Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch [1/250], Loss: 1.4253, Accuracy: 39.70%\nEpoch [2/250], Loss: 1.1386, Accuracy: 50.70%\nEpoch [3/250], Loss: 1.0564, Accuracy: 53.79%\nEpoch [4/250], Loss: 1.0305, Accuracy: 54.86%\nEpoch [5/250], Loss: 1.0059, Accuracy: 55.96%\nEpoch [6/250], Loss: 0.9939, Accuracy: 56.78%\nEpoch [7/250], Loss: 0.9781, Accuracy: 57.96%\nEpoch [8/250], Loss: 1.0522, Accuracy: 55.63%\nEpoch [9/250], Loss: 1.0412, Accuracy: 55.59%\nEpoch [10/250], Loss: 0.9757, Accuracy: 58.30%\nEpoch [11/250], Loss: 0.9583, Accuracy: 58.96%\nEpoch [12/250], Loss: 0.9491, Accuracy: 59.21%\nEpoch [13/250], Loss: 0.9482, Accuracy: 59.34%\nEpoch [14/250], Loss: 0.9409, Accuracy: 59.65%\nEpoch [15/250], Loss: 0.9821, Accuracy: 58.00%\nEpoch [16/250], Loss: 0.9366, Accuracy: 59.62%\nEpoch [17/250], Loss: 0.9370, Accuracy: 59.73%\nEpoch [18/250], Loss: 0.9203, Accuracy: 60.42%\nEpoch [19/250], Loss: 0.9136, Accuracy: 60.66%\nEpoch [20/250], Loss: 0.9101, Accuracy: 60.97%\nEpoch [21/250], Loss: 0.8989, Accuracy: 61.33%\nEpoch [22/250], Loss: 0.8948, Accuracy: 61.31%\nEpoch [23/250], Loss: 0.8975, Accuracy: 61.36%\nEpoch [24/250], Loss: 0.8953, Accuracy: 61.37%\nEpoch [25/250], Loss: 0.8929, Accuracy: 61.46%\nEpoch [26/250], Loss: 0.8863, Accuracy: 61.76%\nEpoch [27/250], Loss: 0.8871, Accuracy: 61.59%\nEpoch [28/250], Loss: 0.8860, Accuracy: 61.72%\nEpoch [29/250], Loss: 0.8833, Accuracy: 61.83%\nEpoch [30/250], Loss: 0.8805, Accuracy: 61.83%\nEpoch [31/250], Loss: 0.8781, Accuracy: 61.91%\nEpoch [32/250], Loss: 0.8778, Accuracy: 62.00%\nEpoch [33/250], Loss: 0.9034, Accuracy: 61.00%\nEpoch [34/250], Loss: 0.8751, Accuracy: 62.05%\nEpoch [35/250], Loss: 0.8755, Accuracy: 62.01%\nEpoch [36/250], Loss: 0.8726, Accuracy: 62.15%\nEpoch [37/250], Loss: 0.8793, Accuracy: 61.90%\nEpoch [38/250], Loss: 0.8669, Accuracy: 62.19%\nEpoch [39/250], Loss: 0.8810, Accuracy: 61.71%\nEpoch [40/250], Loss: 0.8606, Accuracy: 62.52%\nEpoch [41/250], Loss: 0.8682, Accuracy: 62.21%\nEpoch [42/250], Loss: 0.8609, Accuracy: 62.33%\nEpoch [43/250], Loss: 0.8702, Accuracy: 62.09%\nEpoch [44/250], Loss: 0.8570, Accuracy: 62.56%\nEpoch [45/250], Loss: 0.8589, Accuracy: 62.47%\nEpoch [46/250], Loss: 0.8640, Accuracy: 62.29%\nEpoch [47/250], Loss: 0.8611, Accuracy: 62.40%\nEpoch [48/250], Loss: 0.8587, Accuracy: 62.48%\nEpoch [49/250], Loss: 0.8565, Accuracy: 62.49%\nEpoch [50/250], Loss: 0.8571, Accuracy: 62.57%\nEpoch [51/250], Loss: 0.8636, Accuracy: 62.32%\nEpoch [52/250], Loss: 0.8515, Accuracy: 62.77%\nEpoch [53/250], Loss: 0.8553, Accuracy: 62.66%\nEpoch [54/250], Loss: 0.9097, Accuracy: 60.99%\nEpoch [55/250], Loss: 0.8603, Accuracy: 62.46%\nEpoch [56/250], Loss: 0.8589, Accuracy: 62.51%\nEpoch [57/250], Loss: 0.8503, Accuracy: 62.83%\nEpoch [58/250], Loss: 0.8522, Accuracy: 62.67%\nEpoch [59/250], Loss: 0.8767, Accuracy: 61.90%\nEpoch [60/250], Loss: 0.8474, Accuracy: 62.88%\nEpoch [61/250], Loss: 0.8487, Accuracy: 62.90%\nEpoch [62/250], Loss: 0.8494, Accuracy: 62.78%\nEpoch [63/250], Loss: 0.8959, Accuracy: 61.33%\nEpoch [64/250], Loss: 0.8528, Accuracy: 62.73%\nEpoch [65/250], Loss: 0.8500, Accuracy: 62.85%\nEpoch [66/250], Loss: 0.8472, Accuracy: 62.96%\nEpoch [67/250], Loss: 0.8541, Accuracy: 62.65%\nEpoch [68/250], Loss: 0.8501, Accuracy: 62.78%\nEpoch [69/250], Loss: 0.8858, Accuracy: 61.59%\nEpoch [70/250], Loss: 0.8651, Accuracy: 62.26%\nEpoch [71/250], Loss: 0.8489, Accuracy: 62.88%\nEpoch [72/250], Loss: 0.8486, Accuracy: 62.88%\nEpoch [73/250], Loss: 0.8474, Accuracy: 62.96%\nEpoch [74/250], Loss: 0.8492, Accuracy: 62.81%\nEpoch [75/250], Loss: 0.8531, Accuracy: 62.77%\nEpoch [76/250], Loss: 0.8833, Accuracy: 61.92%\nEpoch [77/250], Loss: 0.8528, Accuracy: 62.65%\nEpoch [78/250], Loss: 0.8476, Accuracy: 62.92%\nEpoch [79/250], Loss: 0.8432, Accuracy: 63.09%\nEpoch [80/250], Loss: 0.8444, Accuracy: 62.96%\nEpoch [81/250], Loss: 0.8490, Accuracy: 62.84%\nEpoch [82/250], Loss: 0.8420, Accuracy: 63.06%\nEpoch [83/250], Loss: 0.8476, Accuracy: 62.93%\nEpoch [84/250], Loss: 0.8454, Accuracy: 63.02%\nEpoch [85/250], Loss: 0.8428, Accuracy: 63.00%\nEpoch [86/250], Loss: 0.8446, Accuracy: 63.09%\nEpoch [87/250], Loss: 0.8479, Accuracy: 62.91%\nEpoch [88/250], Loss: 0.8415, Accuracy: 63.10%\nEpoch [89/250], Loss: 0.8469, Accuracy: 62.96%\nEpoch [90/250], Loss: 0.8419, Accuracy: 63.03%\nEpoch [91/250], Loss: 0.8399, Accuracy: 63.21%\nEpoch [92/250], Loss: 0.8543, Accuracy: 62.67%\nEpoch [93/250], Loss: 0.8459, Accuracy: 63.08%\nEpoch [94/250], Loss: 0.8425, Accuracy: 63.10%\nEpoch [95/250], Loss: 0.8473, Accuracy: 62.94%\nEpoch [96/250], Loss: 0.8403, Accuracy: 63.21%\nEpoch [97/250], Loss: 0.8414, Accuracy: 63.06%\nEpoch [98/250], Loss: 0.8383, Accuracy: 63.26%\nEpoch [99/250], Loss: 0.8389, Accuracy: 63.18%\nEpoch [100/250], Loss: 0.8421, Accuracy: 63.03%\nEpoch [101/250], Loss: 0.8542, Accuracy: 62.85%\nEpoch [102/250], Loss: 0.8389, Accuracy: 63.18%\nEpoch [103/250], Loss: 0.8401, Accuracy: 63.12%\nEpoch [104/250], Loss: 0.8399, Accuracy: 63.13%\nEpoch [105/250], Loss: 0.8378, Accuracy: 63.22%\nEpoch [106/250], Loss: 0.8389, Accuracy: 63.19%\nEpoch [107/250], Loss: 0.8378, Accuracy: 63.23%\nEpoch [108/250], Loss: 0.8380, Accuracy: 63.22%\nEpoch [109/250], Loss: 0.9119, Accuracy: 60.68%\nEpoch [110/250], Loss: 0.8441, Accuracy: 63.00%\nEpoch [111/250], Loss: 0.8412, Accuracy: 63.21%\nEpoch [112/250], Loss: 0.8403, Accuracy: 63.23%\nEpoch [113/250], Loss: 0.8448, Accuracy: 63.04%\nEpoch [114/250], Loss: 0.8373, Accuracy: 63.32%\nEpoch [115/250], Loss: 0.8371, Accuracy: 63.30%\nEpoch [116/250], Loss: 0.8368, Accuracy: 63.23%\nEpoch [117/250], Loss: 0.8411, Accuracy: 63.20%\nEpoch [118/250], Loss: 0.8350, Accuracy: 63.34%\nEpoch [119/250], Loss: 0.8915, Accuracy: 61.71%\nEpoch [120/250], Loss: 0.8459, Accuracy: 63.05%\nEpoch [121/250], Loss: 0.8401, Accuracy: 63.14%\nEpoch [122/250], Loss: 0.8396, Accuracy: 63.21%\nEpoch [123/250], Loss: 0.8382, Accuracy: 63.24%\nEpoch [124/250], Loss: 0.8373, Accuracy: 63.32%\nEpoch [125/250], Loss: 0.8365, Accuracy: 63.30%\nEpoch [126/250], Loss: 0.8427, Accuracy: 62.97%\nEpoch [127/250], Loss: 0.8402, Accuracy: 63.20%\nEpoch [128/250], Loss: 0.8357, Accuracy: 63.34%\nEpoch [129/250], Loss: 0.8360, Accuracy: 63.29%\nEpoch [130/250], Loss: 0.8369, Accuracy: 63.27%\nEpoch [131/250], Loss: 0.8395, Accuracy: 63.23%\nEpoch [132/250], Loss: 0.8338, Accuracy: 63.33%\nEpoch [133/250], Loss: 0.8375, Accuracy: 63.32%\nEpoch [134/250], Loss: 0.8357, Accuracy: 63.35%\nEpoch [135/250], Loss: 0.8371, Accuracy: 63.29%\nEpoch [136/250], Loss: 0.8341, Accuracy: 63.35%\nEpoch [137/250], Loss: 0.8377, Accuracy: 63.23%\nEpoch [138/250], Loss: 0.8422, Accuracy: 63.12%\nEpoch [139/250], Loss: 0.8372, Accuracy: 63.29%\nEpoch [140/250], Loss: 0.8348, Accuracy: 63.33%\nEpoch [141/250], Loss: 0.8768, Accuracy: 61.93%\nEpoch [142/250], Loss: 0.8517, Accuracy: 62.85%\nEpoch [143/250], Loss: 0.8383, Accuracy: 63.33%\nEpoch [144/250], Loss: 0.8368, Accuracy: 63.36%\nEpoch [145/250], Loss: 0.8353, Accuracy: 63.41%\nEpoch [146/250], Loss: 0.8370, Accuracy: 63.36%\nEpoch [147/250], Loss: 0.8340, Accuracy: 63.43%\nEpoch [148/250], Loss: 0.8341, Accuracy: 63.37%\nEpoch [149/250], Loss: 0.8335, Accuracy: 63.43%\nEpoch [150/250], Loss: 0.8342, Accuracy: 63.40%\nEpoch [151/250], Loss: 0.8407, Accuracy: 63.19%\nEpoch [152/250], Loss: 0.8350, Accuracy: 63.40%\nEpoch [153/250], Loss: 0.8347, Accuracy: 63.38%\nEpoch [154/250], Loss: 0.8331, Accuracy: 63.41%\nEpoch [155/250], Loss: 0.8332, Accuracy: 63.46%\nEpoch [156/250], Loss: 0.8313, Accuracy: 63.51%\nEpoch [157/250], Loss: 0.8398, Accuracy: 63.25%\nEpoch [158/250], Loss: 0.8359, Accuracy: 63.30%\nEpoch [159/250], Loss: 0.8375, Accuracy: 63.24%\nEpoch [160/250], Loss: 0.8316, Accuracy: 63.44%\nEpoch [161/250], Loss: 0.8379, Accuracy: 63.26%\nEpoch [162/250], Loss: 0.8322, Accuracy: 63.51%\nEpoch [163/250], Loss: 0.8308, Accuracy: 63.52%\nEpoch [164/250], Loss: 0.8302, Accuracy: 63.52%\nEpoch [165/250], Loss: 0.8312, Accuracy: 63.49%\nEpoch [166/250], Loss: 0.8306, Accuracy: 63.53%\nEpoch [167/250], Loss: 0.8327, Accuracy: 63.47%\nEpoch [168/250], Loss: 0.8302, Accuracy: 63.51%\nEpoch [169/250], Loss: 0.8312, Accuracy: 63.48%\nEpoch [170/250], Loss: 0.8331, Accuracy: 63.39%\nEpoch [171/250], Loss: 0.8352, Accuracy: 63.39%\nEpoch [172/250], Loss: 0.8334, Accuracy: 63.44%\nEpoch [173/250], Loss: 0.8352, Accuracy: 63.34%\nEpoch [174/250], Loss: 0.8380, Accuracy: 63.29%\nEpoch [175/250], Loss: 0.8305, Accuracy: 63.49%\nEpoch [176/250], Loss: 0.8292, Accuracy: 63.54%\nEpoch [177/250], Loss: 0.8408, Accuracy: 63.23%\nEpoch [178/250], Loss: 0.8357, Accuracy: 63.34%\nEpoch [179/250], Loss: 0.8324, Accuracy: 63.46%\nEpoch [180/250], Loss: 0.8287, Accuracy: 63.51%\nEpoch [181/250], Loss: 0.8294, Accuracy: 63.60%\nEpoch [182/250], Loss: 0.8294, Accuracy: 63.51%\nEpoch [183/250], Loss: 0.8409, Accuracy: 63.25%\nEpoch [184/250], Loss: 0.8306, Accuracy: 63.42%\nEpoch [185/250], Loss: 0.8283, Accuracy: 63.55%\nEpoch [186/250], Loss: 0.8280, Accuracy: 63.60%\nEpoch [187/250], Loss: 0.8336, Accuracy: 63.40%\nEpoch [188/250], Loss: 0.8303, Accuracy: 63.45%\nEpoch [189/250], Loss: 0.8280, Accuracy: 63.54%\nEpoch [190/250], Loss: 0.8271, Accuracy: 63.62%\nEpoch [191/250], Loss: 0.8281, Accuracy: 63.60%\nEpoch [192/250], Loss: 0.8296, Accuracy: 63.49%\nEpoch [193/250], Loss: 0.8273, Accuracy: 63.56%\nEpoch [194/250], Loss: 0.8468, Accuracy: 63.04%\nEpoch [195/250], Loss: 0.8291, Accuracy: 63.56%\nEpoch [196/250], Loss: 0.8287, Accuracy: 63.55%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluation function\ndef evaluate(model, test_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            _, predicted = torch.max(outputs, 1)\n            _, labels = torch.max(y_batch, 1)\n            total += y_batch.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n\n# Evaluate the model\nevaluate(model, test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(input_dim, hidden_dim)\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.lstm(x)\n        x = x[:, -1, :]  # Take the output from the last time step\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\n# Hyperparameters\ninput_dim = 16  # Based on hex digits (0-9, a-f)\nhidden_dim = 128\noutput_dim = y_train.shape[1]\nnum_layers = 2\ndropout = 0.2\n\n# Model\nlstm_model = LSTMModel(input_dim, hidden_dim, output_dim, num_layers, dropout)\nlstm_model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:26:54.617460Z","iopub.execute_input":"2024-09-07T13:26:54.617770Z","iopub.status.idle":"2024-09-07T13:26:54.673658Z","shell.execute_reply.started":"2024-09-07T13:26:54.617737Z","shell.execute_reply":"2024-09-07T13:26:54.672708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\ndef train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * X_batch.size(0)\n            _, predicted = torch.max(outputs, 1)\n            _, labels = torch.max(y_batch, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        train_accuracy = 100 * correct / total\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/total:.4f}, Accuracy: {train_accuracy:.2f}%')\n\n# Loss, optimizer, and training params\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\nnum_epochs = 20\n\n# Train LSTM model\ntrain_model(lstm_model, train_loader, test_loader, criterion, optimizer, num_epochs=20)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:29:22.468872Z","iopub.execute_input":"2024-09-07T13:29:22.469556Z","iopub.status.idle":"2024-09-07T13:32:00.678665Z","shell.execute_reply.started":"2024-09-07T13:29:22.469512Z","shell.execute_reply":"2024-09-07T13:32:00.677744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation function\ndef evaluate(model, test_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            _, predicted = torch.max(outputs, 1)\n            _, labels = torch.max(y_batch, 1)\n            total += y_batch.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n\n# Evaluate the model\nevaluate(lstm_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:32:02.169329Z","iopub.execute_input":"2024-09-07T13:32:02.169728Z","iopub.status.idle":"2024-09-07T13:32:02.913527Z","shell.execute_reply.started":"2024-09-07T13:32:02.169685Z","shell.execute_reply":"2024-09-07T13:32:02.912402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BiLSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n        super(BiLSTMModel, self).__init__()\n        self.embedding = nn.Embedding(input_dim, hidden_dim)\n        self.bilstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bidirectional LSTM has 2x hidden size\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.bilstm(x)\n        x = x[:, -1, :]  # Take the output from the last time step\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\n# Model\nbilstm_model = BiLSTMModel(input_dim, hidden_dim, output_dim, num_layers, dropout)\nbilstm_model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:32:02.914587Z","iopub.execute_input":"2024-09-07T13:32:02.914874Z","iopub.status.idle":"2024-09-07T13:32:02.938415Z","shell.execute_reply.started":"2024-09-07T13:32:02.914844Z","shell.execute_reply":"2024-09-07T13:32:02.937559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train BILSTM model\ntrain_model(bilstm_model, train_loader, test_loader, criterion, optimizer, num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:43:52.169286Z","iopub.execute_input":"2024-09-07T13:43:52.169671Z","iopub.status.idle":"2024-09-07T13:49:27.202549Z","shell.execute_reply.started":"2024-09-07T13:43:52.169633Z","shell.execute_reply":"2024-09-07T13:49:27.201549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation function\ndef evaluate(model, test_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            _, predicted = torch.max(outputs, 1)\n            _, labels = torch.max(y_batch, 1)\n            total += y_batch.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n\n# Evaluate the model\nevaluate(bilstm_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:53:39.957786Z","iopub.execute_input":"2024-09-07T13:53:39.958418Z","iopub.status.idle":"2024-09-07T13:53:41.471182Z","shell.execute_reply.started":"2024-09-07T13:53:39.958374Z","shell.execute_reply":"2024-09-07T13:53:41.470252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvLSTMModel(nn.Module):\n    def __init__(self, input_dim, embed_dim, hidden_dim, output_dim, num_layers, dropout):\n        super(ConvLSTMModel, self).__init__()\n        self.embedding = nn.Embedding(input_dim, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, hidden_dim, kernel_size=3, padding=1)\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.embedding(x)  # x shape: [batch_size, seq_len, embed_dim]\n        x = x.permute(0, 2, 1)  # Switch to [batch_size, embed_dim, seq_len] for Conv1D\n        x = self.conv(x)  # Apply 1D convolution\n        x = x.permute(0, 2, 1)  # Back to [batch_size, seq_len, hidden_dim]\n        x, _ = self.lstm(x)\n        x = x[:, -1, :]  # Take the output from the last time step\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\n# Model\nconv_lstm_model = ConvLSTMModel(input_dim, embed_dim=64, hidden_dim=128, output_dim=output_dim, num_layers=2, dropout=0.2)\nconv_lstm_model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:53:43.100668Z","iopub.execute_input":"2024-09-07T13:53:43.101384Z","iopub.status.idle":"2024-09-07T13:53:43.120883Z","shell.execute_reply.started":"2024-09-07T13:53:43.101345Z","shell.execute_reply":"2024-09-07T13:53:43.119909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train CONVLSTM model\ntrain_model(conv_lstm_model, train_loader, test_loader, criterion, optimizer, num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:53:45.837276Z","iopub.execute_input":"2024-09-07T13:53:45.837628Z","iopub.status.idle":"2024-09-07T13:56:42.282463Z","shell.execute_reply.started":"2024-09-07T13:53:45.837595Z","shell.execute_reply":"2024-09-07T13:56:42.281437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, test_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            _, predicted = torch.max(outputs, 1)\n            _, labels = torch.max(y_batch, 1)\n            total += y_batch.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n\n# Evaluate the model\nevaluate(conv_lstm_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:56:43.082152Z","iopub.execute_input":"2024-09-07T13:56:43.082462Z","iopub.status.idle":"2024-09-07T13:56:43.875798Z","shell.execute_reply.started":"2024-09-07T13:56:43.082429Z","shell.execute_reply":"2024-09-07T13:56:43.874865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BiGRUModel(nn.Module):\n    def __init__(self, input_dim, num_classes, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.2):\n        super(BiGRUModel, self).__init__()\n        self.embedding = nn.Embedding(input_dim, embed_dim)\n\n        # Bidirectional GRU\n        self.bigru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, \n                            batch_first=True, bidirectional=True, dropout=dropout)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(hidden_dim * 2, 128)  # Multiplied by 2 for bidirectional GRU\n        self.fc2 = nn.Linear(128, num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Embedding\n        x = self.embedding(x)\n\n        # BiGRU\n        gru_out, _ = self.bigru(x)\n\n        # Use the last hidden state from both directions (concatenated)\n        x = gru_out[:, -1, :]\n\n        # Fully connected layers with ReLU and dropout\n        x = self.dropout(torch.relu(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n\n# Define input/output dimensions\ninput_dim = 16  # 16 possible hex digits (0-9, a-f)\nnum_classes = len(df['algo'].unique())\n\n# Create the BiGRU model\nbigru_model = BiGRUModel(input_dim=input_dim, num_classes=num_classes)\n\n# Use GPU if available\nbigru_model = bigru_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:56:56.127184Z","iopub.execute_input":"2024-09-07T13:56:56.127946Z","iopub.status.idle":"2024-09-07T13:56:56.159290Z","shell.execute_reply.started":"2024-09-07T13:56:56.127906Z","shell.execute_reply":"2024-09-07T13:56:56.158268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train BIGRU model\ntrain_model(bigru_model, train_loader, test_loader, criterion, optimizer, num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:57:04.244496Z","iopub.execute_input":"2024-09-07T13:57:04.244886Z","iopub.status.idle":"2024-09-07T14:05:21.098578Z","shell.execute_reply.started":"2024-09-07T13:57:04.244848Z","shell.execute_reply":"2024-09-07T14:05:21.097573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\nevaluate(bigru_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T14:05:21.100191Z","iopub.execute_input":"2024-09-07T14:05:21.100508Z","iopub.status.idle":"2024-09-07T14:05:23.307215Z","shell.execute_reply.started":"2024-09-07T14:05:21.100474Z","shell.execute_reply":"2024-09-07T14:05:23.306340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef hex_to_numeric(hex_string):\n    return [int(c, 16) for c in hex_string]\n\ndef predict_algorithm(encoded_text, model, encoder, device):\n    # Step 1: Convert hex to numeric values\n    numeric_sequence = hex_to_numeric(encoded_text)\n    \n    # Step 2: Pad the sequence to the maximum length\n    maxlen = model.embedding.num_embeddings\n    padded_sequence = pad_sequences([numeric_sequence], maxlen)\n    \n    # Step 3: Convert the sequence to a PyTorch tensor\n    sequence_tensor = torch.tensor(padded_sequence, dtype=torch.long).to(device)\n    \n    # Step 4: Feed the tensor into the model\n    model.eval()\n    with torch.no_grad():\n        output = model(sequence_tensor)\n        _, predicted = torch.max(output, 1)\n    \n    # Step 5: Decode the prediction\n    algo_classes = encoder.categories_[0]\n    predicted_algorithm = algo_classes[predicted.item()]\n    \n    return predicted_algorithm\n\n# Example usage:\nencoded_text = \"7bd65467969434bb72cc0a85ce7ea5186ff66dd381a168fb9c401bbe542e9e23\"\nprint(f\"The predicted encryption algorithm is: {predicted_algorithm}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T13:52:03.675974Z","iopub.execute_input":"2024-09-07T13:52:03.676947Z","iopub.status.idle":"2024-09-07T13:52:03.687880Z","shell.execute_reply.started":"2024-09-07T13:52:03.676896Z","shell.execute_reply":"2024-09-07T13:52:03.686723Z"},"trusted":true},"execution_count":null,"outputs":[]}]}